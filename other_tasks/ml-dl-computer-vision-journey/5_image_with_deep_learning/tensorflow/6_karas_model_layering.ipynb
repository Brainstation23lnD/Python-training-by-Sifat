{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "## What is model?\n",
    "A model is abstractly:\n",
    "- A function that compute something on tensor(a forward pass)\n",
    "- Some variable that can be updated in response to training.\n",
    "- models are made of layers.\n",
    "- Layers are functions with a known mathematical structure that can be reused and have trainable variables.\n",
    "- In Tensorflow, most high level implementations of layers and models such as Keras is built on tf.Model class.\n",
    "\n",
    "## What is Keras?\n",
    "Keras is a high level API for building and training networks designed for ease of use and rapid prototyping in deep learning.\n",
    "- We use keras layers to build models.\n",
    "- tf.keras.layers.Layer is the base for layers with which is build on top of tf.Module\n",
    "\n",
    "## Common layers for Computer Vision:\n",
    "- Convolution Layers; tf.keras.layers.Conv2D\n",
    "- Pooling Layers: MaxPooling2D, AveragePooling2D\n",
    "- Activation Layers: tf.keras.layers.Activation\n",
    "  - relu --> most common hidden layers max(0,x)\n",
    "  - sigmoid --> for multi-class classification, converting raw score in probabilities that sum to 1.\n",
    "  - softmax --> for binary classification outputting a probability between 0 and 1.\n",
    "- Normalize Layers: BatchNormalization - normalize the activations of the previous layers at each batch, meaning its shifts the mean to 0 and scales the variance to 1.\n",
    "- Regulation Layers:  Dropout - Randomly sets a fraction of input units to 0 at each update during training time, which helps prevent co-adaption of neurons.\n",
    "- Flattening Layers: Flatten - Transform the multidimensional output of convolutional/pooling layers into 1d vector. This is necessary before feeding the data into a fully connected Dense layers, as Dense Layers expect 1D array.\n",
    "- Dense(Fully connected) Layers Dense: every neuron in the layer is connected to every neuron in the previous layer.\n",
    "\n",
    "## Sequential Model:\n",
    " A way to build neural networks where layers are stacked linearly one after another.\n",
    " It's a simple and common approach for creating models with a single input and a single output per layer\n",
    " 1. Simple way to build a neural networks\n",
    "2. It allows you to create models by stacking layers one after another in a linear fashion.\n",
    "\n",
    "Limitation:\n",
    "- Strictly Linear: Cannot handle models with non-linear topologies.\n",
    "- No multiple inputs/outputs.\n",
    "- No share layers.\n",
    "- No branching or skip connection\n",
    "\n",
    "We can save and reload model with running further.\n",
    "\n",
    "---\n",
    "Absolutely! নিচে **Image Processing** এর দৃষ্টিকোণ থেকে TensorFlow এর ডীপ লার্নিং এ ব্যবহৃত important layers গুলো **বাংলায়** বুঝিয়ে দিচ্ছি, কখন কেন এবং কীভাবে ব্যবহার হয়, সাথে ডেটার dimension কেমন হয়—সেটাও বলছি।\n",
    "\n",
    "---\n",
    "\n",
    "# ১. Input Layer (ইনপুট লেয়ার)\n",
    "\n",
    "**কেন ও কখন ব্যবহার?**\n",
    "\n",
    "* ডেটা মডেলে ইনপুট হিসেবে যায় এখানে।\n",
    "* Image processing এ, ইনপুট লেয়ার হিসেবে ছবি বা ছবি থেকে পাওয়া ফিচারগুলো মডেলে দেওয়া হয়।\n",
    "\n",
    "**ডেটার dimension:**\n",
    "\n",
    "* ধরো, RGB ছবি 224x224 পিক্সেলের, তাহলে ইনপুট হবে: `(224, 224, 3)`\n",
    "* অর্থাৎ, 224 পিক্সেল হাইট, 224 পিক্সেল উইডথ, 3 = RGB কালার চ্যানেল।\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* একটা ফটো ক্যামেরা থেকে মডেলে পাঠানো হবে।\n",
    "\n",
    "---\n",
    "\n",
    "# ২. Convolutional Layer (Conv2D)\n",
    "\n",
    "**কেন ও কখন ব্যবহার?**\n",
    "\n",
    "* ছবি থেকে ফিচার (যেমন, এজ, টেক্সচার, শেপ) বের করতে।\n",
    "* প্রতিটি filter ছবির একটি অংশের ওপর কাজ করে এবং ফিচার ম্যাপ তৈরি করে।\n",
    "\n",
    "**ডেটার dimension:**\n",
    "\n",
    "* Input: `(height, width, channels)`\n",
    "* Output: `(new_height, new_width, number_of_filters)`\n",
    "* new\\_height ও new\\_width filter size ও stride এর উপর নির্ভর করে কমতে পারে।\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* একটা ছবিতে ‘circle’ খুঁজে বের করা। Conv layer এর filter সেই circle এর edges detect করবে।\n",
    "\n",
    "---\n",
    "\n",
    "# ৩. Pooling Layer (MaxPooling2D / AveragePooling2D)\n",
    "\n",
    "**কেন ও কখন ব্যবহার?**\n",
    "\n",
    "* ফিচার ম্যাপ এর সাইজ কমাতে (downsampling) এবং কম্পিউটেশন কমাতে।\n",
    "* Noise কমানো এবং বেশি গ্লোবাল ফিচার ধরার জন্য।\n",
    "\n",
    "**ডেটার dimension:**\n",
    "\n",
    "* Input: `(height, width, channels)`\n",
    "* Output: `(height / pool_size, width / pool_size, channels)`\n",
    "* Pooling সাধারনত spatial dimensions ছোট করে দেয়, channel অপরিবর্তিত থাকে।\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* 224x224 ছবি থেকে 112x112 করে ছোট করা যাতে মডেল দ্রুত কাজ করতে পারে।\n",
    "\n",
    "---\n",
    "\n",
    "# ৪. Dense Layer (Fully Connected Layer)\n",
    "\n",
    "**কেন ও কখন ব্যবহার?**\n",
    "\n",
    "* শেষের দিকে ফিচার গুলোকে combine করে decision নেয়ার জন্য।\n",
    "* Image classification এর জন্য শেষ লেয়ার হিসেবে বেশি ব্যবহৃত।\n",
    "\n",
    "**ডেটার dimension:**\n",
    "\n",
    "* Input: 1D vector (যদি Conv layer থাকে, তখন Flatten করে 1D করা হয়)\n",
    "* Output: (number\\_of\\_neurons,)\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* Cats vs Dogs image classifier এর শেষ Dense layer 2 neurons (cat, dog) output দিবে।\n",
    "\n",
    "---\n",
    "\n",
    "# ৫. Flatten Layer\n",
    "\n",
    "**কেন ও কখন ব্যবহার?**\n",
    "\n",
    "* Conv বা Pooling থেকে পাওয়া 3D ফিচার ম্যাপ কে 1D vector এ convert করার জন্য।\n",
    "* কারণ Dense layer 1D ইনপুট নেয়।\n",
    "\n",
    "**ডেটার dimension:**\n",
    "\n",
    "* Input: `(height, width, channels)`\n",
    "* Output: `(height * width * channels,)` (1D)\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* Conv থেকে 7x7x64 output পেলে, Flatten করলে 3136 (7*7*64) লম্বা vector হবে।\n",
    "\n",
    "---\n",
    "\n",
    "# ৬. Dropout Layer\n",
    "\n",
    "**কেন ও কখন ব্যবহার?**\n",
    "\n",
    "* Overfitting কমাতে।\n",
    "* Training এর সময় random কিছু neurons কে ignore করে মডেলকে আরো robust বানায়।\n",
    "\n",
    "**ডেটার dimension:**\n",
    "\n",
    "* Input আর output dimension একরকম থাকে।\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* যদি মডেল খুব বেশি train করে validation set এ খারাপ হয়, তখন Dropout ইউজ করা হয়।\n",
    "\n",
    "---\n",
    "\n",
    "# ৭. Normalization Layer (BatchNormalization)\n",
    "\n",
    "**কেন ও কখন ব্যবহার?**\n",
    "\n",
    "* মডেলের training কে দ্রুত আর স্থিতিশীল করতে।\n",
    "* প্রতিটি layer এর input কে normalize করে mean=0, std=1 করার চেষ্টা করে।\n",
    "\n",
    "**ডেটার dimension:**\n",
    "\n",
    "* Input আর output dimension একই থাকে।\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* বড় বড় মডেল যেমন ResNet এ প্রতিটি Conv layer এর পরে BatchNormalization থাকে।\n",
    "\n",
    "---\n",
    "\n",
    "# ৮. Activation Layer (ReLU, Sigmoid, Softmax)\n",
    "\n",
    "**কেন ও কখন ব্যবহার?**\n",
    "\n",
    "* মডেলে non-linearity introduce করার জন্য।\n",
    "* ReLU (Rectified Linear Unit) বেশি ব্যবহৃত কারণ এটা simple আর effective।\n",
    "\n",
    "**ডেটার dimension:**\n",
    "\n",
    "* Input আর output dimension একই থাকে।\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* ReLU: hidden layers এ, Softmax: classification এর শেষ লেয়ারে।\n",
    "\n",
    "---\n",
    "\n",
    "# ৯. Embedding Layer\n",
    "\n",
    "**কেন ও কখন ব্যবহার?**\n",
    "\n",
    "* Image processing এর জন্য কমই ইউজ হয়, NLP তে বেশি।\n",
    "* Categorical data বা text কে dense vector এ রূপান্তর করার জন্য।\n",
    "\n",
    "---\n",
    "\n",
    "## সারসংক্ষেপ (Image Processing এর জন্য Data Dimension Flow):\n",
    "\n",
    "* **Input Layer:** (224,224,3) ছবি\n",
    "* **Conv Layer:** (224,224,3) → (222,222,32) \\[filter size 3x3, 32 filters]\n",
    "* **Pooling:** (222,222,32) → (111,111,32) \\[pool size 2x2]\n",
    "* **Conv + Pooling** আরো কিছুবার\n",
    "* **Flatten:** (7,7,64) → (3136,)\n",
    "* **Dense:** (3136,) → (512,)\n",
    "* **Dropout**\n",
    "* **Dense (Output):** (512,) → (number\\_of\\_classes,)\n",
    "\n",
    "---\n",
    "\n"
   ],
   "id": "4d0cf884a985184d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T08:41:30.272267Z",
     "start_time": "2025-08-11T08:41:30.248985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(10,)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "model.summary()"
   ],
   "id": "6a9b1360a6b9924e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential_5\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_15 (\u001B[38;5;33mDense\u001B[0m)                │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m64\u001B[0m)             │           \u001B[38;5;34m704\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (\u001B[38;5;33mDense\u001B[0m)                │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m32\u001B[0m)             │         \u001B[38;5;34m2,080\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (\u001B[38;5;33mDense\u001B[0m)                │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m10\u001B[0m)             │           \u001B[38;5;34m330\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">704</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m3,114\u001B[0m (12.16 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,114</span> (12.16 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m3,114\u001B[0m (12.16 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,114</span> (12.16 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a2dff35a496ad4dc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
